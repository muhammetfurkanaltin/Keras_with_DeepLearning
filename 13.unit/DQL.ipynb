{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derin Pekiştirmeli Ogrenme / Deep q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install gym\n",
    "# %pip install pygame\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:00<00:00,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, time : 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, time : 10\n",
      "time : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 2\n",
      "time : 3\n",
      "time : 4\n",
      "time : 5\n",
      "time : 6\n",
      "time : 7\n",
      "time : 8\n",
      "time : 9\n",
      "time : 10\n",
      "time : 11\n",
      "time : 12\n",
      "time : 13\n",
      "time : 14\n",
      "time : 15\n",
      "time : 16\n",
      "time : 17\n",
      "time : 18\n",
      "time : 19\n",
      "time : 20\n",
      "time : 21\n",
      "time : 22\n",
      "time : 23\n",
      "time : 24\n",
      "time : 25\n",
      "time : 26\n",
      "time : 27\n",
      "time : 28\n",
      "time : 29\n",
      "time : 30\n",
      "time : 31\n",
      "time : 32\n",
      "time : 33\n",
      "time : 34\n",
      "time : 35\n",
      "time : 36\n",
      "time : 37\n",
      "time : 38\n",
      "time : 39\n",
      "time : 40\n",
      "time : 41\n",
      "time : 42\n",
      "time : 43\n",
      "time : 44\n",
      "time : 45\n",
      "time : 46\n",
      "time : 47\n",
      "time : 48\n",
      "time : 49\n",
      "time : 50\n",
      "time : 51\n",
      "time : 52\n",
      "time : 53\n",
      "time : 54\n"
     ]
    }
   ],
   "source": [
    "class DQLAgent:\n",
    "    def __init__(self, env):\n",
    "        #cevrenin gözlem alanı (state) boyutu\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "\n",
    "        #cevrede bulunan eylem sayisi (ajanın secebilcegi eylem sayisi)\n",
    "        self.action_size = env.action_space.n\n",
    "\n",
    "        #gelecekteki odullerin indirim orani\n",
    "        self.gamma = 0.95\n",
    "\n",
    "        self.learning_rate = 0.001\n",
    "\n",
    "        self.epsilon = 1.0\n",
    "\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "        self.epsilon_min = 0.01\n",
    "\n",
    "        self.memory = deque(maxlen=1000)\n",
    "\n",
    "        self.model = self.build_model() # ANN\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(48, input_dim = self.state_size, activation=\"relu\"))\n",
    "        model.add(Dense(24, activation=\"relu\"))\n",
    "        model.add(Dense(self.action_size, activation=\"linear\"))\n",
    "        model.compile(loss = \"mse\", optimizer = Adam(learning_rate = self.learning_rate))\n",
    "\n",
    "        return model\n",
    "\n",
    "    def remember(self,state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self,state):\n",
    "        \n",
    "        if random.uniform(0,1) <= self.epsilon:\n",
    "            return env.action_space.sample() # rastgle eylem seç\n",
    "        \n",
    "        act_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(act_values[0])\n",
    "    def replay(self,batch_size):\n",
    "        \n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            if done : # eger done ise bitis durum var ise odulu dogrudan hedef olarak aliriz\n",
    "                target = reward \n",
    "            else:\n",
    "                target = reward + self.gamma*np.amax(self.model.predict(next_state, verbose=0)[0])\n",
    "\n",
    "            # modelin tahmin ettigi oduller   \n",
    "            train_target = self.model.predict(state, verbose=0)\n",
    "\n",
    "            #ajanın yaptıgı eyleme gore tahmin edilen odulu guncelle\n",
    "            train_target[0][action] = target\n",
    "\n",
    "            #modeli egit \n",
    "            self.model.fit(state, train_target, verbose =0)\n",
    "\n",
    "    def adaptiveEGreedy(self):\n",
    "\n",
    "        self.epsilon > self.epsilon_min \n",
    "        self.epsilon = self.epsilon * self.epsilon_decay\n",
    "\n",
    "# %% env kullanarak dql ajanı baslatma\n",
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\") # cartpole ortamı baslatma\n",
    "agent = DQLAgent(env)\n",
    "\n",
    "batch_size = 32 # egitim için minibatch boyutu\n",
    "episodes = 2 # epochs, simulasyonun oynatılacagı toplam bolum sayisi\n",
    "\n",
    "for e in tqdm(range(episodes)):\n",
    "\n",
    "    # ortamı sıfırla baslangıc durumunu al \n",
    "    state = env.reset()[0] # ortamı sıfırlamak \n",
    "    state = np.reshape(state, [ 1,4])\n",
    "    time = 0 # zamanı adimi baslat\n",
    "    \n",
    "    while True:\n",
    "        # ajan eylem secer \n",
    "        action = agent.act(state)\n",
    "\n",
    "        # ajanımız ortamda bu eylemi uygular ve bu eylem sonucunda next_state,reward , bitis bilgisi(done) alir\n",
    "        (next_state, reward, done , _ , _ ) = env.step(action)\n",
    "        next_state = np.reshape(state,[1,4])\n",
    "\n",
    "        #yapmıs oldugu bu adimi yani eylemi ve bu eylem sonucu env alinan bilgileri kaydeder \n",
    "        agent.remember(state,action,reward,next_state,done)\n",
    "\n",
    "        # mevct durumu gğnceller\n",
    "        state = next_state\n",
    "\n",
    "        # deneyimleerden yeniden oynatmayı baslatır reply() => tarning\n",
    "        agent.replay(batch_size)\n",
    "\n",
    "        # epsilonu set eder \n",
    "        agent.adaptiveEGreedy()\n",
    "\n",
    "        # zaman adimini arttırır\n",
    "        time = time + 1\n",
    "\n",
    "        # eger done ise donguyu kirar ve bolum biter ve yeni bolume baslar \n",
    "        if done: \n",
    "            print(f\"Episode: {e}, time : {time}\")\n",
    "            break\n",
    "\n",
    "# test edilmesi \n",
    "import time \n",
    "\n",
    "trained_model = agent \n",
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\") # cartpole ortamı baslatma\n",
    "\n",
    "state = env.reset()[0]\n",
    "state = np.reshape(state, [1,4])\n",
    "\n",
    "time_t = 0 \n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    action = trained_model.act(state)\n",
    "    (next_state, reward, done , _ , _ ) = env.step(action)\n",
    "    next_state = np.reshape(next_state, [1,4])\n",
    "    state = next_state \n",
    "    time_t += 1 \n",
    "    print(f\"time : {time_t}\")\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if done:\n",
    "        break \n",
    "        print (\"Done\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
